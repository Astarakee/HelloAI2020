{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -=-=-=-=-=-=-=Support Vector Machines-=-=-=-=-=-=-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A) Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supprt Vector Machines (SVM) are a powerful class of supervised classification and also regression. In this exercise, we will review the SVM practically and use it for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose here is to define a line in 2D or a manifold in multi-dimensions to divide classes of the data. For instance, consider the following simple case of a 2D data set which are linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Data/Data0.csv', header=None)\n",
    "X = data.values[:, :2]\n",
    "y = data.values[:, 2]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this simple example, there are two classes highlighted with different colors. The classes do not have overlap and therefore you can even use a linear equation to separate them from each other.. However, the problem is that there are unlimited number of such lines that are able to separate the classes. Based on which lines you choose,  a new data point will be either below or above the line and will be assigned to one of the classes. (The point marked by \"X\" in the following plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-0.5, 3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
    "\n",
    "# The following script draws a line by following the equation \"Y-y0 = m(X-x0)\" \n",
    "# on the data plane. Please note the valu inside each of the parentheses\n",
    "# represent the parameters of 'm' and 'b' of the equation. \n",
    "for m, b in [(-0.67, 2.2), (-1.5,2.5)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.plot([2.5],[0],'X',color='red', markeredgewidth=1, markersize=8)\n",
    "\n",
    "plt.xlim(-0.5, 3);\n",
    "\n",
    "# TODO: Was the point \"X\" assigned to the same class when you used differen separating line?\n",
    "# How do you justify such a discrepancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It would be possible that the new data point was assigned to a different class based on the line you draw. One way to address such problems is to use two separating lines instead of only one line and maximize the distance between the two separating lines (maximize the margins). In fact, using only one line for separation is like to use two lines with a zero-width of margin. Therefore, we can add a margin between the separating lines and the line that maximizes this margin can be selected as an optimal line in SVM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs  # Loading the input data\n",
    "X, y = make_blobs(n_samples=50, centers=2,random_state=0, cluster_std=0.60)\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
    "\n",
    "# Comparing to the previous lines, here another parameter \"d\" is added.\n",
    "# The parameter 'd' works as a bias to move the lines up and down.\n",
    "# The following parameters of 'd' inside the parentheses are found experimentally.\n",
    "# What SVM does, is finding the such parameters automatically.\n",
    "\n",
    "for m, b, d in [ (0.5, 1.6, 0.6), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.6)\n",
    "\n",
    "plt.xlim(-1, 3.5);\n",
    "\n",
    "# TODO: Change the parameters 'd' inside the parentheses (0.6 and 0.2) and\n",
    "# notice how it would change the margins. Please note the margins should not, optimally,\n",
    "# include any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, instead of finding the line and margin width manually, we are going to train a model by using a linear SVM to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)  # Set the linear kernel and a large value of C\n",
    "model.fit(X, y) # Fitting the data set to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To understand what SVM does, it is better to visualize how it find the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='r',\n",
    "               levels=[-1, 0, 1], alpha=0.8,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot the support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=500, linewidth=10, facecolor='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data point distributions:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
    "# Drawing the decision boundaries.\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the plot above, there is one solid line (main separating line) and two dashed-lines (margins). Any other lines will have less values of margin than the one above. Some of the training data points in this plot are placed exactly on the margin lines. In other words, margin lines passed through the middle of these points. These important points are called \"Support Vectors\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the exact number of Support Vectors.\n",
    "n_support = model.support_vectors_\n",
    "print('The number of support vector is: {}'.format(len(n_support)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In SVMs, finding the optimum separating line  depends on the position (coordinates) of these Support Vectors. Other points further from the margins are in the correct sides and do not effect the fitting procedure. That is because these points do not contribute to the loss function used to fit the model, so their position and their frequency do not matter as long as they do not cross the margin. We can observe such behavior, for example, if we plot the model learned from the first 70 points and, then, first 140 points of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [70, 140]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above two figures depict that SVM model fits the line using only Support Vectors and the model is not sensitive to other data points. In fact, the left figure contains half of the data points of the right figure, but both of them have the same separating lines because both of them have similar Support Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the number of the data points and interpret the results.\n",
    "from ipywidgets import interact, fixed\n",
    "interact(plot_svm, N=[8,10,20,50,100,150,180,300,500,2000], ax=fixed(None));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about non-linear separable data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles  # Load input data\n",
    "X, y = make_circles(50, factor=.1, noise=0.2)\n",
    "\n",
    "Model1 = SVC(kernel='linear').fit(X, y) # Fit a linear SVM\n",
    "\n",
    "# Plot the data points distribution using the \"plt.scatter\"\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
    "\n",
    "# Call the function \"plot_svc_decision_function\" to draw the decision boundaries.\n",
    "plot_svc_decision_function(Model1);\n",
    "\n",
    "# TODO: Run this cell several times; are the drawn lines able to properly separate the classes?why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could you observe acceptable results? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One practical way to handle nonlinear separable data set is to use \"Kernel Trick\". This means we can project our data set to a higher dimension space where the data points are linearly separable. There are different functions for this projection such as Gaussian (Radial) basis function and polynomials. In other words, we map a nonlinear separable data set into a higher dimension space, artificially, then within the new space linear separation would be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run and play with the angles\n",
    "# If you want separate the two classes in higher dimenstion linearly, which line will you use?\n",
    "r = np.exp(-(X ** 2).sum(1))\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=60, cmap='viridis')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=[-90,-60,0,30,90], azip=(-180,180),\n",
    "         X=fixed(X), y=fixed(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Kernel-based SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model2 = SVC(kernel='rbf', C=1E6) # Creating an SVM with Radial Basis Function\n",
    "Model2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data points distribution using the \"plt.scatter\"\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap='viridis')\n",
    "\n",
    "# Call the function \"plot_svc_decision_function\" to draw the decision boundaries.\n",
    "plot_svc_decision_function(Model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin SVMs: So far we have seen data set with a clear decision boundary; however, in real cases data points from different classes often overlapping each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of dataset with class overlapping\n",
    "X, y = make_blobs(n_samples=200, centers=2,random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis');\n",
    "# TODO: Is it possible to use linear or even kernel based SVM to classify these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In previous examples there were a parameter \"C\" which we set it as a very large number. The role of this parameter is to prevent the data points from appearing inside the margins. The larger this value the lower the chance of data points inside the margins. Accordingly, if we do not allow any points to appear inside the margins we have a \"Hard Margin SVM\" like what we did in previous examples. On the other hand, we can allow the margins to be softer to contain some of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of soft and hard margin SVM.\n",
    "X, y = make_blobs(n_samples=100, centers=2,random_state=0, cluster_std=0.8) # Input Data\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='viridis')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)\n",
    "    \n",
    "# TODO: How do you compare these two figures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Multi-Class Classification Problem Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea is to train an SVM classifier using Iris data set with different Kernels. Iris data set contains 4 features for each data point including Sepal length, Sepal width, Petal length and Petal width all in cm. The target class of these data are Setosa, Versicolor, Virginica. Therefore, it is a 3 class classification problem where each data point is described with 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets # To Get iris dataset\n",
    "from sklearn import svm    # To fit the svm classifier\n",
    "iris_dataset = datasets.load_iris()\n",
    "print (\"Iris data set Description = \", iris_dataset['DESCR'])\n",
    "print (\"Iris targets = \", iris_dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can visualize different features with the corresponding targets. For instance, visualizing Sepal length & width and the corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visuvalize_sepal_data():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:, :2]  # we only take the first two features.\n",
    "    y = iris.target\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.title('Sepal Width & Length')\n",
    "    plt.show()\n",
    "visuvalize_sepal_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous cells but for the last two features. (Petal length and width)\n",
    "def visuvalize_petal_data():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:, 2:]  # we only take the last two features.\n",
    "    y = iris.target\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "    plt.xlabel('Petal length')\n",
    "    plt.ylabel('Petal width')\n",
    "    plt.title('Petal Width & Length')\n",
    "    plt.show()\n",
    " \n",
    "visuvalize_petal_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling different kernel SVM by using only the Sepal features (Length and Width) and the Petal features (Lenght and Width) separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the Sepal two features.\n",
    "y = iris.target\n",
    "C = 1  # SVM regularization parameter\n",
    " \n",
    "# SVM with linear kernel\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "\n",
    "# the same with \"rbf\" kernel\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.5, C=C).fit(X, y)\n",
    "\n",
    "\n",
    "# the same with \"poly\" kernel (polynomial)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=4, C=C).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  In order to understand how well the SVM models classified the data, we can visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading\n",
    "# create a mesh to plot in\n",
    "h = .02  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "\t                     np.arange(y_min, y_max, h))\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "\t    'SVC with RBF kernel',\n",
    "\t    'SVC with polynomial (degree 3) kernel']\n",
    " \n",
    " \n",
    "for i, clf in enumerate((svc, rbf_svc, poly_svc)):\n",
    "\t # Plot the decision boundary. For that, we will assign a color to each\n",
    "\t # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\t plt.subplot(3, 1, i + 1)\n",
    "\t plt.subplots_adjust(wspace=0.4, hspace=0.8)\n",
    " \n",
    "\t Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    " \n",
    "\t # Put the result into a color plot\n",
    "\t Z = Z.reshape(xx.shape)\n",
    "\t plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.8)\n",
    " \n",
    "\t # Plot also the training points\n",
    "\t plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "\t plt.xlabel('Sepal length')\n",
    "\t plt.ylabel('Sepal width')\n",
    "\t plt.xlim(xx.min(), xx.max())\n",
    "\t plt.ylim(yy.min(), yy.max())\n",
    "\t plt.xticks(())\n",
    "\t plt.yticks(())\n",
    "\t plt.title(titles[i])\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the parameters C, standard deviation of RBF kernels (gammas) and polynomial order (degree)\n",
    "# and find out how they effect the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same process for Iris Petal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, 2:]  # we only take the last two features.\n",
    "y = iris.target\n",
    "C = 1.0  # SVM regularization parameter\n",
    " \n",
    "# SVC with linear kernel\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "# SVC with RBF kernel\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.5, C=C).fit(X, y)\n",
    "# SVC with polynomial (degree 3) kernel\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "\t            np.arange(y_min, y_max, h))\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "\t  'SVC with RBF kernel',\n",
    "\t  'SVC with polynomial (degree 3) kernel']\n",
    " \n",
    " \n",
    "for i, clf in enumerate((svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.8)\n",
    " \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    " \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.8)\n",
    " \n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "    plt.xlabel('Petal length')\n",
    "    plt.ylabel('Petal width')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# C) Train and Test with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We are going to use a real clinical data set (Data1.csv). This data set contain 216 subjects each of which presented with 100 quantified values (clinical Proteomics). Out of all the subjects, 121 are diagnosed as ovarian cancer and 95 are healthy subjects. Therefore, it is binary classification and the class labels are set as -1 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('./Data/Data1.csv', header=None) # Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose is to divide the data set into train and test set, using the training data to learn the model and evaluate the performance of the learned model on the unseen, test, data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the first 100 columns as input data\n",
    "X = data.values[:, :100]\n",
    "\n",
    "# Set the last column as targets\n",
    "y = data.values[:, 100]\n",
    "\n",
    "# spliting the data into train and test sets randomly.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a SVM with training data\n",
    "C = 1\n",
    "Model4 = SVC(kernel='linear', C=C) \n",
    "Model4.fit(X_train,y_train) \n",
    "\n",
    "# Predicting the label of the unseen data\n",
    "y_pred = Model4.predict(X_test) \n",
    "\n",
    "print('Prediction Accuracy Score is:',metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the previous cell for 'rbf' kernel\n",
    "Model5 = SVC(kernel='rbf',gamma=0.1, C=C)\n",
    "Model5.fit(X_train,y_train)\n",
    "y_pred = Model5.predict(X_test)\n",
    "print('Prediction Accuracy Score is:',metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Repeat the whole previous cell for 'poly' kernel\n",
    "Model6 = SVC(kernel='poly',degree=2, C=C)\n",
    "Model6.fit(X_train,y_train)\n",
    "y_pred = Model6.predict(X_test)\n",
    "print('Prediction Accuracy Score is:',metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing K-fold (k=5) cross validation with Linear kernel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "Model7 = SVC(kernel='linear')\n",
    "# TODO: play with the parameter cv and find out how it works.\n",
    "scores = cross_val_score(Model7, X, y, cv=5, scoring='accuracy') #cv is cross validation\n",
    "print(scores)\n",
    "\n",
    "# TODO: How cross validation would help the classification task?\n",
    "# TODO: Can you explain the achieved scores in terms of bias-variance matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As an another basic example, we are going to classify MR breast images from MR bladder images. The images were rescaled to 40*40 pixels and the pixel values were used as the data. Therefore, each subject is presented by a vector with length of 1600 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the input data: \"RawPixels.csv\"\n",
    "data = pd.read_csv('./Data/RawPixels.csv', header=None)\n",
    "X = data.values[:, :1600] # input data\n",
    "y = data.values[:, 1600] # targets\n",
    "\n",
    "# Splitting the data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "C = 1\n",
    "Model9 = SVC(kernel='linear', C=C)\n",
    "Model9.fit(X_train,y_train)\n",
    "\n",
    "y_pred = Model9.predict(X_test) \n",
    "print('The Accuracy Score is:',metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Good Luck"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
